Output when slowmodel=True

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

Loss:  50693950.0
0: 9.317977905273438
1: 0.7169268131256104
2: 0.7172749042510986
3: 0.7169392108917236
4: 0.7170450687408447
5: 0.7164778709411621
6: 0.716156005859375
7: 0.7194457054138184
8: 0.7200524806976318
9: 0.7192277908325195
Loss:  48070976.0
10: 0.7171323299407959
11: 0.7173082828521729
12: 0.7180063724517822
13: 0.7177290916442871
14: 0.7172956466674805
15: 0.7153463363647461
16: 0.7157344818115234
17: 0.7172420024871826
18: 0.7161448001861572
19: 0.7165243625640869
Loss:  44700600.0
20: 0.71915602684021
21: 0.7165300846099854
22: 0.7182989120483398
23: 0.718191385269165
24: 0.7164618968963623
25: 0.7179105281829834
26: 0.7169990539550781
27: 0.718794584274292
28: 0.7187917232513428
29: 0.716496467590332
Loss:  40704404.0
30: 0.7158412933349609
31: 0.7162158489227295
32: 0.7179255485534668
33: 0.7178859710693359
34: 0.7186167240142822
35: 0.7192654609680176
36: 0.7150392532348633
37: 0.7152142524719238
38: 0.7163543701171875
39: 0.716374397277832
Loss:  36991836.0
40: 0.7174434661865234
41: 0.7193048000335693
42: 0.7150266170501709
43: 0.7198152542114258
44: 0.7143204212188721
45: 0.7193474769592285
46: 0.7178370952606201
47: 0.7172055244445801
48: 0.7156851291656494
49: 0.7202582359313965
Loss:  34993770.0
50: 0.7185742855072021
51: 0.7135803699493408
52: 0.720097541809082
53: 0.7152488231658936
54: 0.7172975540161133
55: 0.7214035987854004
56: 0.7158398628234863
57: 0.7181878089904785
58: 0.7193558216094971
59: 0.7198348045349121
Loss:  34384056.0
60: 0.7195315361022949
61: 0.7196698188781738
62: 0.7209131717681885
63: 0.7192654609680176
64: 0.7149484157562256
65: 0.714684247970581
66: 0.7158036231994629
67: 0.7211263179779053
68: 0.7144944667816162
69: 0.7165040969848633
Loss:  33926660.0
70: 0.7195496559143066
71: 0.718407392501831
72: 0.714669942855835
73: 0.7180593013763428
74: 0.738605260848999
75: 0.7409045696258545
76: 0.7167994976043701
77: 0.7304749488830566
78: 0.723567008972168
79: 0.7190418243408203
Loss:  33409680.0
80: 0.7154140472412109
81: 0.7161471843719482
82: 0.7160851955413818
83: 0.7133316993713379
84: 0.7152602672576904
85: 0.7153551578521729
86: 0.7183530330657959
87: 0.7160117626190186
88: 0.7179274559020996
89: 0.7155165672302246
Loss:  32734586.0
90: 0.7189724445343018
91: 0.7223470211029053
92: 0.7172715663909912
93: 0.7166798114776611
94: 0.7160215377807617
95: 0.7175426483154297
96: 0.7168104648590088
97: 0.7147161960601807
98: 0.7152433395385742
99: 0.7166974544525146
Loss:  31844304.0
100: 0.7189052104949951
101: 0.7161376476287842
102: 0.7177212238311768
103: 0.7182998657226562
104: 0.7178215980529785
105: 0.7186601161956787
106: 0.7176132202148438
107: 0.7187454700469971
108: 0.7162983417510986
109: 0.7167375087738037
Loss:  30793766.0
110: 0.7174901962280273
111: 0.7167003154754639
112: 0.719677209854126
113: 0.7174558639526367
114: 0.7156391143798828
115: 0.7191212177276611
116: 0.7170538902282715
117: 0.7146446704864502
118: 0.7172427177429199
119: 0.7192621231079102
Loss:  29776216.0
120: 0.7171695232391357
121: 0.7177488803863525
122: 0.7175657749176025
123: 0.7167840003967285
124: 0.7197723388671875
125: 0.7156758308410645
126: 0.7162086963653564
127: 0.71793532371521
128: 0.7141356468200684
129: 0.7194559574127197
Loss:  28977014.0
130: 0.7164959907531738
131: 0.7182736396789551
132: 0.7187495231628418
133: 0.7182488441467285
134: 0.7148656845092773
135: 0.7195124626159668
136: 0.7189149856567383
137: 0.7166733741760254
138: 0.7190823554992676
139: 0.7164409160614014
Loss:  28436588.0
140: 0.7183554172515869
141: 0.7158758640289307
142: 0.7205958366394043
143: 0.7165768146514893
144: 0.7206103801727295
145: 0.7190375328063965
146: 0.7179572582244873
147: 0.7155628204345703
148: 0.7209737300872803
149: 0.7171182632446289
Loss:  28091558.0
150: 0.7170906066894531
151: 0.7176117897033691
152: 0.7195117473602295
153: 0.718297004699707
154: 0.7190594673156738
155: 0.7185616493225098
156: 0.7199993133544922
157: 0.7190661430358887
158: 0.7177097797393799
159: 0.719954252243042
Loss:  27869206.0
160: 0.7180416584014893
161: 0.7221879959106445
162: 0.7211430072784424
163: 0.7185568809509277
164: 0.7157607078552246
165: 0.7169830799102783
166: 0.7193913459777832
167: 0.7186193466186523
168: 0.7161412239074707
169: 0.7159104347229004
Loss:  27720506.0
170: 0.7186005115509033
171: 0.7151198387145996
172: 0.7161321640014648
173: 0.7176151275634766
174: 0.7160463333129883
175: 0.7160937786102295
176: 0.7187395095825195
177: 0.7208132743835449
178: 0.7167081832885742
179: 0.7175390720367432
Loss:  27616982.0
180: 0.7139246463775635
181: 0.7166836261749268
182: 0.7201864719390869
183: 0.7154495716094971
184: 0.7176041603088379
185: 0.7202234268188477
186: 0.714867115020752
187: 0.717181921005249
188: 0.7156369686126709
189: 0.7172403335571289
Loss:  27542372.0
190: 0.7175674438476562
191: 0.7174952030181885
192: 0.7149789333343506
193: 0.7186479568481445
194: 0.7160508632659912
195: 0.7165987491607666
196: 0.71551513671875
197: 0.716219425201416
198: 0.7160186767578125
199: 0.7161760330200195
Loss:  27487020.0
200: 0.7147560119628906
