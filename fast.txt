Output when slowmodel=False


WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

Loss:  42603364.0
0: 10.275982141494751
1: 0.3545703887939453
2: 0.35489416122436523
3: 0.35672879219055176
4: 0.35543107986450195
5: 0.35530519485473633
6: 0.35573768615722656
7: 0.35555171966552734
8: 0.35492825508117676
9: 0.3554391860961914
Loss:  36053156.0
10: 0.35584568977355957
11: 0.3553433418273926
12: 0.3552377223968506
13: 0.3556253910064697
14: 0.35529661178588867
15: 0.3549525737762451
16: 0.35544800758361816
17: 0.3541836738586426
18: 0.354661226272583
19: 0.35486412048339844
Loss:  34496590.0
20: 0.3556396961212158
21: 0.3555893898010254
22: 0.35529661178588867
23: 0.35564279556274414
24: 0.35558128356933594
25: 0.35524606704711914
26: 0.35564327239990234
27: 0.35542821884155273
28: 0.3548622131347656
29: 0.3560512065887451
Loss:  31679918.0
30: 0.3558659553527832
31: 0.3559722900390625
32: 0.35602569580078125
33: 0.3564276695251465
34: 0.3554825782775879
35: 0.3548712730407715
36: 0.3554050922393799
37: 0.35535478591918945
38: 0.3558375835418701
39: 0.35561394691467285
Loss:  28834678.0
40: 0.35598134994506836
41: 0.3559868335723877
42: 0.35530567169189453
43: 0.3555796146392822
44: 0.35598278045654297
45: 0.3554978370666504
46: 0.3556497097015381
47: 0.3557453155517578
48: 0.3546326160430908
49: 0.35755157470703125
Loss:  27729346.0
50: 0.35442447662353516
51: 0.3550410270690918
52: 0.35454750061035156
53: 0.3547656536102295
54: 0.35471081733703613
55: 0.354703426361084
56: 0.3549613952636719
57: 0.35457634925842285
58: 0.35446810722351074
59: 0.35486721992492676
Loss:  27512542.0
60: 0.3554728031158447
61: 0.35495710372924805
62: 0.355130672454834
63: 0.35527682304382324
64: 0.35483789443969727
65: 0.3555588722229004
66: 0.35599589347839355
67: 0.3562793731689453
68: 0.35632920265197754
69: 0.3560957908630371
Loss:  27408198.0
70: 0.35628628730773926
71: 0.3558032512664795
72: 0.35750770568847656
73: 0.356201171875
74: 0.35593128204345703
75: 0.3558368682861328
76: 0.3561866283416748
77: 0.3557136058807373
78: 0.3561224937438965
79: 0.3557910919189453
Loss:  27357382.0
80: 0.35573601722717285
81: 0.35570716857910156
82: 0.3558340072631836
83: 0.35550928115844727
84: 0.3556227684020996
85: 0.3554518222808838
86: 0.3562746047973633
87: 0.3554956912994385
88: 0.3557255268096924
89: 0.3558475971221924
Loss:  27385920.0
90: 0.3559892177581787
91: 0.35575151443481445
92: 0.3558375835418701
93: 0.35635805130004883
94: 0.3560926914215088
95: 0.3562431335449219
96: 0.3558347225189209
97: 0.35597896575927734
98: 0.35553693771362305
99: 0.3557446002960205
Loss:  27317782.0
100: 0.3563809394836426
101: 0.35591602325439453
102: 0.35591745376586914
103: 0.3555936813354492
104: 0.355419397354126
105: 0.35397934913635254
106: 0.35495948791503906
107: 0.354874849319458
108: 0.354860782623291
109: 0.35477709770202637
Loss:  27365370.0
110: 0.35520195960998535
111: 0.354630708694458
112: 0.35509490966796875
113: 0.3550262451171875
114: 0.3550548553466797
115: 0.35595059394836426
116: 0.3545358180999756
117: 0.35506558418273926
118: 0.35575127601623535
119: 0.3553793430328369
Loss:  27329156.0
120: 0.356032133102417
121: 0.35549116134643555
122: 0.35446834564208984
123: 0.35646533966064453
124: 0.3556671142578125
125: 0.35523056983947754
126: 0.3568441867828369
127: 0.3560464382171631
128: 0.3547389507293701
129: 0.3543820381164551
Loss:  27323200.0
130: 0.35625529289245605
131: 0.35567474365234375
132: 0.3555641174316406
133: 0.35643577575683594
134: 0.35507774353027344
135: 0.35547351837158203
136: 0.35523462295532227
137: 0.35526609420776367
138: 0.3546736240386963
139: 0.3549027442932129
Loss:  27286104.0
140: 0.3549003601074219
141: 0.35529565811157227
142: 0.35523009300231934
143: 0.3558523654937744
144: 0.355452299118042
145: 0.354921817779541
146: 0.35610413551330566
147: 0.3556368350982666
148: 0.3556373119354248
149: 0.3554248809814453
Loss:  27365558.0
150: 0.35584044456481934
151: 0.35483598709106445
152: 0.3551182746887207
153: 0.35472607612609863
154: 0.35527944564819336
155: 0.35511016845703125
156: 0.35558032989501953
157: 0.3555448055267334
158: 0.35540032386779785
159: 0.3555614948272705
Loss:  27282034.0
160: 0.35431575775146484
161: 0.355316162109375
162: 0.35512757301330566
163: 0.35573530197143555
164: 0.3555262088775635
165: 0.3558943271636963
166: 0.35520219802856445
167: 0.35549092292785645
168: 0.3554353713989258
169: 0.3554971218109131
Loss:  27276484.0
170: 0.3548588752746582
171: 0.3565483093261719
172: 0.3548557758331299
173: 0.3557703495025635
174: 0.35531115531921387
175: 0.354231595993042
176: 0.3543367385864258
177: 0.354888916015625
178: 0.3549370765686035
179: 0.3555021286010742
Loss:  27325972.0
180: 0.35480642318725586
181: 0.35632824897766113
182: 0.3558034896850586
183: 0.35550475120544434
184: 0.35633277893066406
185: 0.3562655448913574
186: 0.35630083084106445
187: 0.3568882942199707
188: 0.35675501823425293
189: 0.35671067237854004
Loss:  27297946.0
190: 0.3571350574493408
191: 0.35477638244628906
192: 0.35539960861206055
193: 0.35544371604919434
194: 0.35509634017944336
195: 0.3554689884185791
196: 0.35559535026550293
197: 0.3549220561981201
198: 0.35520386695861816
199: 0.3552665710449219
Loss:  27277428.0
200: 0.35515499114990234
